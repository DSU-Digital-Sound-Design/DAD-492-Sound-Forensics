+++
title = "Standard operating procedures and Bias"
outputs = ["Reveal"]
[reveal_hugo]
theme = "solarized"
show_notes = "separate-page"
+++

## Standard operating procedures

>  documents created by laboratories detailing specific methodologies to ensure the scientific principles of repeatability and reproducibility are met.

[Procedures for Evidence Collection, Handling, and Storage](https://fws.gov/policy/e1445fw3.pdf)

{{% note %}}

* ensures error mitigation
* reduced liability of examiners actions, 
* consistency 
* quality assurance 

--- 

## Bias Mitigation 

> You must not fool yourself, and you are the easiest person to fool.

(Fenyman, 1964)

{{% note %}}

* Bias can come in at any stage
* because they can be subtle they can creep in without the examiner noticing 
* aka unconscious bias 
* this is why there are so many rules put in place 
* here are a few types of bias 

--- 

## Forensic Confirmation Bias

> the class of effects through which an individual’s pre-existing beliefs, expectations, motives and situational content influence the collection, perception and interpretation of evidence during the course of a criminal case.

Kassin et al. (2013)

{{% note %}}
* conscious or subconsciously 
* often introduced from extraneous information being provided to the examiner; outside the scope of the investigation 
* if an examiner is told certain information about a this can influence their opinion about the evidence 
* they might expect a certain outcome to be true 
* another bias, base rate or expected norm
    *  A prime example is a peer reviewer who, over time, begins to expect to agree with the findings of another examiner’s report if they rarely disagree (and thus assume the base rate).
    *  peer review can help mitigate this bias
    *  dummy cases with incorrect data can also be used  


---

## Base-rate fallacy

> people’s tendency to ignore base rates in favour of individuating information rather than integrating the two

(Bar-Hillel, 1980)

{{% note %}}

The base rate fallacy, also called base rate neglect or base rate bias, is a type of fallacy. If presented with related base rate information and specific information, people tend to ignore the base rate in favor of the individuating information, rather than correctly integrating the two. Base rate neglect is a specific form of the more general extension neglect.

* if a witness is 95% accurate it does not mean that their next testimony will be 95% accurate


---

## Prosecutor's fallacy 

{{% note %}}

* Allows the prosecutor to exaggerate the probability of a defendants guilt. 
* A positive result in the test may paradoxically be more likely to be an erroneous result than an actual occurrence, even if the test is very accurate. 
* example: a defendant has a given blood type shared by 10% of the population, the prosecutor would say that the defendant has a 90% chance of being guilty. 
* this ignores the probability of the defendant being innocent 
* Consider, for instance, that 1000 people live in the town where the murder occurred. This means that 100 people live there who have the perpetrator's blood type; therefore, the true probability that the defendant is guilty – based on the fact that his blood type matches that of the killer – is of only 1%, which is much less than the 90% argued by the prosecutor. 
  
---

## Texas sharpshooter’s fallacy

> the act of giving significance to random data after an event has occurred.

{{% note %}}

It takes its name from the story of a Texan who fired a rifle into a barn before painting around each of the bullet holes to falsely demonstrate the perceived accuracy of the shot. It is also known as painting the target around the arrow (Thompson, 2009).

An example in audio forensics would be adjusting the criteria for a match within a speaker comparison examination after the analysis has taken place to improve results (in effect, moving the target to match the outcome). To a third party, this may appear to improve the accuracy of the analysis, while it has, in fact, reduced it, as the criteria required to result in a match have been broadened.

--- 

## Optimism bias

> the difference between a person’s expectation and the outcome that follows

Sharot (2011)

{{% note %}}

can be caused by an examiner’s optimism in the methodologies and tools used. For example, applying a noise reduction process to an audio recording in the belief that the overall quality has improved when it has, in fact, reduced the intelligibility of the speech.

--- 

## Contextual Bias 

> well-intended experts are vulnerable to making erroneous decisions by extraneous influences

Venville (2011) 

{{% note %}}

ex:  knowing the conclusions of other investigations relating to a case, such as the results of a DNA test relating to a subject for whom a voice comparison exercise has been requested.

Framing bias - Framing bias is a form of contextual bias where an examiner only analyses a section of the information available (hence ‘framing’), and draws their conclusions from such without consideration for the rest of the data. 

Statistical bias -  the method causes errors due to the data analysed, the analysis procedure, or the interpretation of the analysis
* an analysis procedure which is not fit for purpose is used, such as using a method to analyse the compression history of a file for which it is not designed
* using an incorrect reference population when performing a voice comparison examination.


---

## Allegiance bias/adversarial allegiance

> the presumed tendency for experts to reach conclusions that support the party who retained them, and as such, they are not, in fact, independent

(Murrie et al., 2013)

{{% note %}}

* Theories for this range from 
    * experts beginning to think of themselves as being on a specific team (Brodsky, 2012), 
    * unintentional and known cognitive errors such as confirmation biases (Neal and Grisso, 2014), 
    * intentional processes and motives for financial gain (Hagen, 1997).
  

--- 

## Mitigating bias

* limiting access to evidence for examiners
* Linear Sequential Unmasking

{{% note %}}

* If you're having a hard time keeping track of these biases it might be because there can be a great deal of overlap between them. 
    * Contextual bias can lead to confirmation bias 
    * because most biases are unconscious, best practices are very important 
    * then peer review is important to catch bias even when best practices were followed 
    * one good way to mitigate bias is to only provide the examiner the case material that is relevant to their investigation, and only provide more information on the case if requested by the examiner 
    * Linear sequential unmasking - By splitting the work process into distinct stages and then only making available the information required by an examiner at each stage, the potential for some of the biases is minimised. 